# 2025-09-16 -- Cannabis Compliance: AI Automation
> Implementing IDE Scratchpad Habit

---

## üìù Daily Preface
- **Why Dev Helper matters here:** It‚Äôs not *required* for compliance, but it‚Äôs my **sandbox** to rapidly prototype audit automation (prompts, parsing, schema validation) before wiring into SaaS.  
- **Two layers of testing:**  
  1. **Lower-level (Dev Helper)** ‚Üí validate AI automation logic itself (prompt building, parsing, OpenAI comms).  
  2. **Higher-level (Cannabis-Compliance)** ‚Üí validate system integration (API routes, controllers, services, full response).  
- **Core takeaway:**  
  - Dev Helper = **Lab (fast prototyping + learning)**  
  - Cannabis-Compliance = **Deployment (production system)**  

---

## üîó Audit Automation Testing Layers

Dev Helper (Lab / Unit-Level)                Cannabis-Compliance (Deployment / Integration)
----------------------------                 ----------------------------------------------
 CLI Command:                                API Request (Postman/cURL):
   dev-helper compliance analyze-audit         POST /api/audit/summarize
         |                                               |
         v                                               v
 analyzeAuditCommand                             auditAutomationController
         |                                               |
 analyzeAuditHandlers                             auditAutomationService
         |                                               |
 ------------------------------- Shared AI Automation Layer -------------------------------
         |                                               |
 Prompt Builder (wraps logs)                      Prompt Builder (wraps logs)
 Call OpenAI API ‚Üí Response                       Call OpenAI API ‚Üí Response
 Response Parser ‚Üí Schema                         Response Parser ‚Üí Schema
 ------------------------------- ------------------------------- ---------------------------
         |                                               |
 Console output (prototype)                        JSON response to client
 (dev-facing feedback loop)                        (regulator / dashboard)

---

## Note-taking Reference Guide

### Tag Entries
Use simple markdown marks while jotting:
- `CONCEPT` ‚Üí something belongs in [Concepts](../concepts/)
- `DECISION` ‚Üí something belongs in [ADR](../architecture/adr/)
- `HOWTO` ‚Üí [Recipe](../guides/recipes/)
- `OPEN` -> open question/backlog
- **Example**:
    - `CONCEPT`: Audit Logs need to link every compliance gate ‚Üí belongs in concepts/audit-logs.md
    - `DECISION`: Will start AI automation with single-log analysis ‚Üí ADR candidate
    - `HOWTO`: Recipe for adding new compliance gate to automation pipeline
    - `OPEN`: How to handle training verification in Phase 1?

### Graduate Notes into Docs on Milestones
At the end of a feature or learning sprint:
- Scan `journal/` for `CONCEPT:` ‚Üí promote to `/concepts`
- Scan for `DECISION`: ‚Üí write ADR (template already exists)
- Scan for `HOWTO`: ‚Üí stub or complete a recipe
- Copy `OPEN`: into docs/Backlog.md

### Use ADRs & Backlog as Anchors
- **ADRs** capture decisions (so we don‚Äôt re-litigate later).
- **Backlog.md** captures ‚ÄúI should doc this later‚Äù without breaking our flow.
- This prevents us from derailing into ‚Äúdoc-OCD‚Äù mid-build.

### Daily Wrap-Up Habit
At the end of a coding/learning block:
1. Commit code as usual.
2. Copy/paste our scratch notes into docs/journal/YYYY-MM-DD.md.
3. Add a quick commit like:
```makefile
docs: journal notes for Sept 15 (AI Automation Phase 1 kickoff)
```
That way, even if we never polish them, our thought process is versioned and searchable.
4. Copy unresolved OPEN items into docs/Backlog.md.

---

## Guide Use-Case Example: Audit Automation (`/audit-automation/analyze`)

- **HOWTO**: Route will take auditlog JSON ‚Üí send to OpenAI ‚Üí return enriched log
- **DECISION**: MVP will only support single log entry (not batch)
- **CONCEPT**: AI summaries can serve as compliance "lens" across audit logs

### End of day:
- Promote the `DECISION` into `adr/0003-ai-automation-mvp.md`.
- Leave the `HOWTO` in journal for now ‚Üí stub later into `recipes/Analyze-AuditLogs.md`.
- The `CONCEPT` eventually grows into `concepts/ai-automation.md`.

---

## Note Canvas (9/16)

### Why were involving Dev-Helper w/ Audit Automation Feature
- Compliance commands in **Dev Helper** exist as a sandbox to **prototype compliance-specific AI workflows** before migrating them into the Cannabis-Compliance SaaS
- Purpose of the `commands/compliance/` folder:  
  - Keep compliance logic separate from core dev commands.  
  - Mirror Dev Helper‚Äôs command/handler structure for consistency.  
  - Allow faster iteration using Dev Helper‚Äôs existing plumbing (CLI, prompts, OpenAI integration, error handling, history).  
- Workflow approach:  
  1. **Prototype in Dev Helper** ‚Äî run commands like `dev-helper compliance analyze-audit ./logs/sample.json`.  
  2. **Refactor into Cannabis-Compliance** ‚Äî move stable logic into `auditAutomationService`, controllers, and routes.  
  3. **Deploy in SaaS** ‚Äî production for regulators, QA, and dashboards.  
- Key takeaway:  
  - **Dev Helper = Lab (fast prototyping)**  
  - **Cannabis-Compliance = Deployment (real users)**  
- Benefits: rapid iteration, code reuse, and validated workflows before wiring into compliance gates.

### Testing the AI Automation Feature (Audit Logs)
1. **External Service (Postman / cURL)**
- Use Postman or cURL to hit the **Cannabis-Compliance backend route** (e.g., `POST /api/audit/summarize`).
- We send it raw audit log data, and it flows:
```nginx
API route ‚Üí auditAutomationController ‚Üí auditAutomationService ‚Üí AI Automation Layer ‚Üí OpenAI ‚Üí response
```
- This validates:
    - API routing is correct
    - Controller ‚Üí service wiring work
    - OpenAI API calls succeeds
    - Response is shaped correctly
üëâ Great for **black-box** testing of the whole stack (no CLI involvement).

2. **Dev Helper CLI**
- Run something like:
```bash
dev-helper compliance analyze-audit ./logs/sample.json
```
- This flows:
```objectivec
CLI ‚Üí analyzeAuditCommand ‚Üí analyzeAuditHandlers ‚Üí AI Automation Layer ‚Üí OpenAI ‚Üí response
```
- This validates:
    - CLI command and handler logic
    - Prompt building
    - Response parsing/schema validation
    - Communication with OpenAI in a controlled, **developer-first sandbox**
üëâ Great for **prototyping and iterating** on prompt quality and output format without needing the full SaaS.
---
### Audit Automation Feature -- Two Layers of Testing
**(1) Lower-Level Component (inside Dev Helper or shared module)**
- **Scope**: Prompt building, response parsing/schema validation, OpenAI communication.
- **Purpose**: Validate the *AI Automation logic itself* works before it ever touches our Cannabis-Compliance API.
- **Why Dev Helper Helps**:
    - We already have CLI + OpenAI integration set up.
    - We can rapidly iterate on prompts, schemas, and handlers without full SaaS overhead.
    - We build muscle memory for *AI workflow design* (inputs ‚Üí prompts ‚Üí outputs)
- **Test Example**: Run `dev-helper compliance analyze-audit ./logs/sample.json` and check:
    - Does the prompt structure make sense?
    - Does the response parse into the schema correctly?
    - Are errors/logging handled?
üëâ Think of this as **unit-level prototyping** for our AI automation layer.

**(2) Higher-Level Component (inside Cannabis-Compliance SaaS)**
- **Scope**: API routing, controller ‚Üí service wiring, OpenAI API call execution, response shape returned to the client.
- **Purpose**: Validate the *system integration* -- ensuring the end-to-end flow works when the API is hit externally.
- **Why Postman/API Testing Helps**:
    - Tests the real request lifecycle (client ‚Üí API ‚Üí service ‚Üí OpenAI ‚Üí response).
    - Ensure JSON formatting, headers, authentication, etc. are correct.
    - Simulates what real users/regulators will see.
- **Test Example**: Use Postman to `POST /api/audit/summarize` with sample audit logs and confirm:
    - Does the route trigger the right controller?
    - Does `auditAutomationService` run correctly?
    - Does the final response match compliance schema?
üëâ Think of this as **integration/system testing** for the deployed app.

---

### Clarifying Confusion from Yesterday
- We've got:
    - `analyzeAuditCommand` ‚Üí the CLI entrypoint.
    - `analyzeAuditHandlers` ‚Üí where prompt building, API call, and response parsing live.
- We've also got `projectConfig` and its `projectEndpoints`. This is the part of Dev Helper that tells our CLI where to send requests (e.g., to our backend `/analyze`, `/history`, etc.).

**Do We Need a New Route in Dev Helper?**
It depends on how we want to test:

**Option A -- Direct CLI ‚Üí OpenAI (No New Route)**
- The CLI handler (`analyzeAuditHandlers`) calls the OpenAI API **directly**, just like `analyze` does in its early versions.
- This makes sense if were keeping the compliance prototype **contained in Dev Helper.**
- **Pro**: Fast, no backend wiring needed
- **Con**: We bypass our Dev Helper backend, so we're not simulating the full API-style flow.
üëâ Use this if we just want the CLI to ‚Äútalk straight to OpenAI‚Äù for prototyping.

**Option B -- CLI ‚Üí Dev Helper Backend ‚Üí OpenAI**
- We create a new route in our Dev Helper backend (e.g., `/compliance/analyzeAudit`).
- The CLI command calls that endpoint (through `projectEndpoints`), and the backend controller/service then handles the OpenAI call.
- **Pro**: Closer to how our compliance SaaS will behave (CLI ‚Üí API ‚Üí service ‚Üí OpenAI).
- **Con**: More setup (new controller, service, route in backend).
üëâ Use this if we want to **prototype the full integration pattern** before porting to Cannabis-Compliance.

‚úÖ **Approach: Option A first**
Since we already have `analyzeAuditCommand` + `analyzeAuditHandlers`, and our focus right now is **prompt building / schema validation**, we don't *need* to create a new route.
- Stick w/ **Option A** first (CLI direct ‚Üí OpenAI).
- Later, if we want to simulate the full stack, we'll add **Option B** with a new backend route.

üìå On `projectEndpoints`
- If we stay with **Option A**, we don't need to add a `projectEndpoints` entry for `analyzeAudit`. The handler can just invoke our shared AI Automation layer directly.
- If we went with **Option B**, then yes--we'd add a new endpoint like:
```json
"projectEndpoints": {
  "analyze": "http://localhost:3001/analyze",
  "history": "http://localhost:3001/history",
  "analyzeAudit": "http://localhost:3001/compliance/analyzeAudit"
}
```

### Wiring up Option A

üìÇ File: `commands/compliance/analyzeAuditHandlers.js`
```js
import fs from "fs";
import path from "path";
import OpenAI from "openai";

// initialize OpenAI client
const client = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

/**
 * Reads an audit log file and sends it to OpenAI for analysis.
 * @param {string} filePath - Path to the audit log JSON file.
 * @returns {Promise<object>} structured compliance summary
 */
export async function handleAnalyzeAudit(filePath) {
  try {
    // 1. Load audit log JSON
    const absPath = path.resolve(process.cwd(), filePath);
    const fileContent = fs.readFileSync(absPath, "utf-8");
    const auditLog = JSON.parse(fileContent);

    // 2. Build the prompt
    const prompt = `
      You are an AI compliance assistant. Summarize the following audit log entry 
      for a regulator. Include:
      - A plain-language summary
      - Any detected deviations
      - Which compliance gates it maps to
      
      Respond in strict JSON with keys: summary, deviations, mappedGates.

      Audit Log Data:
      ${JSON.stringify(auditLog, null, 2)}
    `;

    // 3. Call OpenAI API
    const response = await client.chat.completions.create({
      model: "gpt-4.1",
      messages: [{ role: "user", content: prompt }],
      temperature: 0.2,
    });

    const raw = response.choices[0].message.content;

    // 4. Parse JSON safely
    let structured;
    try {
      structured = JSON.parse(raw);
    } catch (err) {
      throw new Error(`Failed to parse OpenAI response as JSON: ${raw}`);
    }

    // 5. Return structured response
    return structured;
  } catch (err) {
    console.error("‚ùå Error in handleAnalyzeAudit:", err.message);
    throw err;
  }
}
```

üìÇ **File**: `commands/compliance/analyzeAuditCommand.js`
```js
import { handleAnalyzeAudit } from "./analyzeAuditHandlers.js";

export default async function analyzeAuditCommand(filePath) {
  try {
    const result = await handleAnalyzeAudit(filePath);
    console.log("‚úÖ Compliance Summary:");
    console.log(JSON.stringify(result, null, 2));
  } catch (err) {
    console.error("‚ùå Failed to analyze audit log:", err.message);
  }
}
```

üñ•Ô∏è **Usage**
```bash
dev-helper compliance analyze-audit ./logs/sample.json
```

**Sample Test: `logs/sample.json`**
```json
{
    "id": "log-001",
    "timestamp": "2025-09-15T14:23:00Z",
    "user": "jane.doe",
    "action": "updated_sop",
    "resource": "SOP-Quality-Control-v2",
    "details": {
      "previousVersion": "v1",
      "newVersion": "v2",
      "changeReason": "Updated procedure for equipment calibration",
      "reviewedByQA": false
    },
    "location": "Processing Facility - Room A",
    "system": "Document Management System"
  }
  
```

‚úÖ Why This Works
- **Direct to OpenAI** ‚Üí no backend route needed.
- **Rapid iteration** ‚Üí we can tweak the prompt, schema, and handler easily.
- **Learning benefit** ‚Üí we see the *exact piece* of the automation logic in action.

### Testing Option A
```bash
PS C:\Users\mattg\dev-helper-ai> dev-helper compliance analyze-audit ./logs/sample.json
‚úÖ Compliance Summary:
{
  "summary": "On September 15, 2025, user jane.doe updated the Standard Operating Procedure (SOP) for Quality Control from version 1 to version 2 in the Document Management System at Processing Facility - Room A. The update was made to revise the procedure for equipment calibration.",
  "deviations": "The updated SOP was not reviewed by Quality Assurance (QA) prior to implementation, as indicated by the 'reviewedByQA' field being false. This may violate standard change control and document review requirements.",
  "mappedGates": [
    "Document Change Control",
    "Quality Assurance Review",
    "SOP Management"
  ]
}
```
#### Result
‚úÖ **Compliance Audit Automation Command is officially working inside Dev Helper**
What just happened:
- CLI reads `./logs/sample.json`
- `handleAnalyzeAudit` wrapped it into a compliance-specific prompt.
- OpenAI returned a **structured JSON summary** (summary, deviations, mapped gates).
- Output printed cleanly in our terminal.
#### Takeaway
- ‚úÖ **End-to-end flow works** (CLI ‚Üí OpenAI ‚Üí structured output).
- ‚úÖ **Lazy client init fixed the env issue.**
- ‚úÖ **Root .env shared** between CLI + server.
- ‚úÖ **Compliance sandbox is alive** inside Dev Helper.
#### Next Steps
**1. Schema Validation**
- Wrap the AI response in Zod (or similar) to enforce structure.
- Prevents malformed JSON from sneaking through.

**2. Batch Support**
- Extend handler to process an array of logs instead of single entries.
- E.g. `dev-helper compliance analyze-audit ./logs/batch.json.`

**3. History Integration**
- Store audit analyses in MongoDB alongside dev commands.
- Queryable by date, user, compliance gate, etc.

**4. Move Logic into Server** (Option B)
- Add a POST /compliance/analyzeAudit route to your backend.
- CLI then calls server instead of OpenAI directly ‚Üí simulates the SaaS.

---

## OPEN Items (Carried Forward)
- Schema validation for AI responses (Zod integration)
- Batch audit log support
- History integration for compliance analyses
- Decide timing of moving logic from Dev Helper ‚Üí Cannabis-Compliance backend

---

## Reflection / Next Steps
- ‚úÖ Validated Dev Helper compliance CLI end-to-end with sample log
- ‚úÖ Fixed env + lazy init issues
- üéØ Next focus: strengthen reliability (schema validation) before scaling to batch or server integration
- üóìÔ∏è Tomorrow: draft scaffolding for `auditAutomationService.js` and `auditAutomationController.js` in Cannabis-Compliance repo